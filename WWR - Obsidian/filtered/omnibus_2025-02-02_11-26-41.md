# 🔹 202502021111.md 🔹




# 🔹 202502021107.md 🔹
```markdown
# A Child-Centered Approach to Local Knowledge

## Overview
I’m developing a framework that organizes community knowledge around **clusters of approximately 150 people**, each anchored by a small group of children. These children are the common focus for parents, teachers, neighbors, and other caretakers, forming a *living micro-community* whose shared experience can be tracked and preserved in a knowledge graph.

## Why Children?
Many of our core ideas and values are **formed in childhood**—language, social roles, even what counts as a “job” or a “skill.” By focusing on a child’s immediate network (families, teachers, grandparents, etc.), we capture how local meanings take shape **at the very stage** where people learn them most directly.

## The 150-Person Cluster
- **Manageable Size**: Around 150 people (sometimes called “Dunbar’s number”) is small enough for direct interaction yet large enough to show real diversity in jobs, skills, and perspectives.
- **Time-Bound Cohort**: As children age, the cluster evolves—some families leave, new caretakers join—giving us a natural cycle to observe changing definitions of “education,” “work,” and “community support.”

## What Gets Collected?
1. **Local Definitions** of “job,” “skill,” “education,” etc.—specifically how these words are used day-to-day by the cluster.  
2. **Individual Data**: Each participant’s story (e.g. current job, skill set, education background).  
3. **Relational Links**: Who collaborates with whom? Which teachers connect with which parents? How do neighbors pool resources?

## Publishing via GRC-20
- **Structured Yet Adaptive**: We use GRC-20 to store entities (like `Person`, `Job`, `Skill`) and their relationships.  
- **Versioning**: Meanings drift over time, so the system can evolve as local definitions change—yet still preserve older versions.  
- **Interoperability**: Other communities or apps can tap into the same GRC-20 standard to compare data or collaborate, without forcing a one-size-fits-all approach.

## Why This Matters
- **Local Accuracy**: By capturing how people *actually* use language, we avoid top-down definitions that might not reflect real lived experience.  
- **Community-Building**: A child-focused cluster naturally motivates cooperation, making it a rich source of genuine data about jobs, resources, and education paths.  
- **Long-Term Preservation**: Over multiple years, we’ll see how local norms evolve, providing a unique record of community adaptation.

## Next Steps
- **Define an Ontology**: Start with a minimal schema for “Person,” “Child,” “Job,” and “Skill.”  
- **Pilot a Cluster**: Collect real data from one 150-person group.  
- **Iterate & Scale**: As we learn from the pilot, refine the schemas and invite more clusters to join.

---

*By focusing on children and their immediate network, this project aims to **preserve local nuance** and capture the true evolution of communal knowledge over time. It’s a sociologically grounded, open-ended framework that can be replicated in neighborhoods everywhere—each cluster a self-contained window into how real people define and manage everyday life.*
```



# 🔹 202502021049.md 🔹
Below is an updated set of points under “Why Local Knowledge Matters,” **with an additional focus on the role of childhood** in shaping language and meaning.

---

## 3. **Why Local Knowledge Matters**

1. **Meanings Drift**  
    Over time, the same word (e.g., “job,” “education,” “qualification”) can take on different connotations in different neighborhoods or subcultures. What you call a “skill” in one context might not match the “skill” definition in another.
    
2. **Local Context**  
    Real knowledge about people’s situations is highly context-specific—especially for things like job searches, skill development, or educational paths. Understanding local norms and relationships often matters more than any general or global definitions.
    
3. **Community as “Opt-In”**  
    The best way to define a community is by who _opts in_ and behaves in a way that reaffirms shared meaning. _Forced_ membership or top-down definitions often mismatch reality. When people join a group voluntarily, they shape and uphold its definitions more authentically.
    
4. **Language Learned in Childhood**  
    Crucially, many core meanings and shared understandings **originate in early childhood** and then get carried forward through life. By tying knowledge structures to a specific cohort of children (as in a CCU), you capture the very stage at which language and communal norms are formed and internalized. That focus on childhood fosters a deeper, more grounded continuity of local meaning.



# 🔹 202502021046.md 🔹
## 3. **Why Local Knowledge Matters**

- **Meanings Drift**: Over time, the same word (e.g., “job,” “education,” “qualification”) can take on different connotations in different neighborhoods or subcultures.
- **Local Context**: Real knowledge about people’s skills and situations is highly context-specific—especially for things like job search or skill development.
- **Community as “Opt-In”**: The best way to define a community is by who _opts in_ and behaves in a way that reaffirms shared meaning. _Forced_ membership or top-down definitions often mismatch reality.
-----

MEaning is tied to the language and understandings that originate with us as children. We learn them and carry them forward.

So - the strcture of language and knowledge being tied to a cohort of children makes sense

--------





# 🔹 202502021040.md 🔹
Below is a concise overview of your background **and** how it naturally leads into the kind of **local knowledge system** you want to build. It also includes some practical suggestions for how you might structure and implement your ideas—especially if you plan to create a data/knowledge system (like a GRC-20 hackathon project) that captures local communities, jobs, education, and evolving meanings.

---

## 1. **Your Background in a Nutshell**

1. **Academic Training**
    
    - _Sociologist, specialized in social network analysis (ABD)._
    - Interest in the **behavior** of groups, how “community” emerges from shared understanding, and how meaning changes over time.
2. **Professional Journey**
    
    - Transitioned from academia to a **landlord/business** role.
    - Experienced running a “humanitarian” business that exposed real-world complexities of local community needs and relationships.
3. **Core Belief**:
    
    - **Preserving unique knowledge** about how people live and relate locally is crucial.
    - Meanings and definitions shift over time; systems that capture “local knowledge” have to remain flexible and iterative.
4. **Research & Observational Insights**
    
    - Studied favelas, personal relationships, and the “cliquish” nature of places like Notre Dame.
    - Believes that local contexts (families, neighborhoods, small economic units) shape how people define everyday concepts like “job,” “skill,” “opportunity,” etc.

---

## 2. **Your Project Vision (CCU, EEU, DDC)**

### 2.1. The CCU (Childcare Unit) Concept
[[202502021045 - uccinct definition and explanation of what a **CCU**]]

- **CCU** = a “cluster” of ~150 people (think: 36 babies, 6 teachers, 72 parents, grandparents, etc.).
- This is your **basic organizing unit** for analyzing or mapping a community. The assumption is that ~150 is a manageable human-scale grouping where direct interactions define meaning.

### 2.2. The Purpose of CCUs

- You want to measure and express local relationships: _who has what job skills, how do parents collaborate, how do they coordinate childcare, etc._
- By capturing knowledge **within** each CCU, you can produce a system that’s **locally accurate** but also standardized enough to compare across multiple CCUs.

### 2.3. The EEU (eastside economic union) and DDC (the dingadongacron)

- You mention “DDC from the EEU” as a _narrow-scope_ organizational framework. Each “EEU” might serve some function (like local education) but is not meant to scale to “everyone on the planet.”
- Instead, you envision _many_ local clusters, each with specialized knowledge and common structures.

---

## 3. **Why Local Knowledge Matters**

- **Meanings Drift**: Over time, the same word (e.g., “job,” “education,” “qualification”) can take on different connotations in different neighborhoods or subcultures.
- **Local Context**: Real knowledge about people’s skills and situations is highly context-specific—especially for things like job search or skill development.
- **Community as “Opt-In”**: The best way to define a community is by who _opts in_ and behaves in a way that reaffirms shared meaning. _Forced_ membership or top-down definitions often mismatch reality.

---

## 4. **How Your Experience Translates into a Knowledge System**

### 4.1. Defining “Jobs” and “Skills” in a CCU

- You want a system that asks, for example:
    
    > _“Hello—we are developing a local economic unit. Please describe your current employment situation and skills that you think are relevant.”_
    
- Then, the system collects the raw text/language each participant uses to describe their work or skills, storing it as **base data**.

### 4.2. Iterative & Adaptive

- After collecting freeform inputs (“I’m a part-time barber,” “I do childcare at home,” “I’m learning coding”), the system **clusters** common phrases or categories.
- Over time, it can update the **ontology**—the structured categories of “job type,” “skill set,” “location”—to reflect how people _actually_ talk.
- This means your data structure (or “schema” if you’re using GRC-20 or any knowledge-graph approach) is **not static**. It must adapt as local usage changes.

### 4.3. Linking to a Broader Model

- Each CCU’s data could link to global references—like a standardized notion of “Barber” or “Childcare provider”—but still store local synonyms or language.
- You can compare CCUs to see how group A’s understanding of “part-time job” might differ from group B’s.

---

## 5. **Practical Steps to Build It**

If you were implementing this as a _Web3 knowledge system_ (e.g., for a GRC-20 hackathon or a broader knowledge-graph project), here’s a simplified approach:

1. **Create a Base Ontology**
    
    - Start with “Person” entities, “Skill” entities, and “Job” entities.
    - Keep them flexible: for example, _Person hasMany Skill_, or _Job references Skill + Organization + Hours_, etc.
2. **Gather Local Data (CCU Scale)**
    
    - For each baby’s family cluster (the CCU), gather real answers to open-ended questions about “employment” and “education.”
    - _Store raw text_ plus minimal structure (who said it, when).
3. **Use an AI or Manual Tagging**
    
    - Over time, label or classify these raw texts. E.g., “Barber,” “Childcare,” “Teacher,” “Software developer.”
    - Each label becomes a _structured entity_ in your knowledge graph, but keep the original text as well.
4. **Publish with Version Control**
    
    - Because meanings drift, you want a versioned system. Each “Job” or “Skill” type can evolve over time (v1, v2, v3).
    - GRC-20 or a similar standard can store _which version_ of a concept your data references.
5. **Governance & Opt-In**
    
    - Let each CCU define membership: only those who “opt in” have editing or viewing privileges.
    - This aligns with your idea that real community membership is proven by participation rather than an abstract label.
6. **Feedback Loop**
    
    - The system _recommends_ new categories or merges similar categories based on user input.
    - People in the CCU can approve or reject these merges.
7. **Cross-Comparison**
    
    - With multiple CCUs, you can see how local definitions differ. For instance, CCU #1 lumps “Self-employed barbers” and “Hair stylists” together as “Hair-care.” CCU #2 sees them as distinct.
    - Over time, that data is valuable for sociological analysis or bridging local business opportunities.

---

## 6. **How This Reflects Your Sociological Insights**

- **Localism & “Focus on 150 People”**: Aligns with well-known sociological ideas of stable group sizes (similar to Dunbar’s number). You’ve created CCUs specifically for that.
- **Meaning Emerges from Practice**: By collecting actual expressions from participants, you see how “job” or “education” is _really_ used.
- **Preserving “Unique Knowledge”**: Instead of forcing a one-size-fits-all dictionary, you store each CCU’s raw data, but can still cross-link it with a _global reference layer_.
- **Time & Drift**: You anticipate that every 3–4 years, each CCU’s membership or definitions might change. Your system must handle that (e.g., versioning, governance, new synonyms).

---

## 7. **Key Points for Implementation Success**

8. **Clear “Base Entities”**: Even if they’re minimal, define `Person`, `Skill`, `Job`, `Organization` in a way that can be updated.
9. **Incremental Data Gathering**: Start small. Let actual responses shape your categories rather than deciding them all upfront.
10. **Versioned Governance**: If you’re using a blockchain/knowledge graph approach, design how “changes” get approved. This fosters trust and preserves the history of definitions.
11. **Privacy vs. Openness**: People might not want their local info fully public. Consider how you’ll store _public vs. private data_ (e.g., encryption, permissioned spaces).
12. **Interoperability**: Use a standard like GRC-20 or any composable knowledge format so that in the future, these local data sets can be aggregated or compared.

---

# Summary

You come from a strong sociological background focusing on **group behavior** and the **drift in shared meaning**. You’re envisioning a system that:

- Organizes people in **CCUs** (~150 people),
- Collects local data on **jobs, education, and skills**,
- Iterates definitions as the community itself evolves,
- Preserves both _raw language_ and _structured categories_ so you can compare across communities.

**In a Web3 or knowledge-graph hackathon context**, you’d implement this by creating a **flexible ontology** (for jobs, skills, etc.), collecting real local data (text + minimal structure), and publishing it in a format that allows for versioned updates. Over time, each CCU would refine its own definitions while still linking to global references. That is how you preserve unique local knowledge **and** enable broader comparisons—exactly the sociological sweet spot you’re after.



# 🔹 202502021045 - uccinct definition and explanation of what a **CCU**.md 🔹
Below is a succinct definition and explanation of what a **CCU** (short for “Cluster of 150 people” or “Child-Centric Unit”) is, based on the context you’ve provided:

---

## **Definition of a CCU**

A **CCU** is a **local, tightly-knit group of around 150 people** (for example, 36 babies, 6 teachers, 72 parents, and grandparents or additional caretakers) who come together around the **shared focus** of raising or educating a small cohort of children. It functions as the **fundamental building block** for community organization and analysis, especially in your framework for measuring and describing local communities.

---

## **Key Characteristics**

1. **Size (~150 People)**
    
    - Roughly aligned with sociological ideas that ~150 is a manageable group size.
    - It’s big enough to encompass meaningful diversity (parents, teachers, extended family), but small enough for close relationships and shared context.
2. **Child-Centric**
    
    - The core unit revolves around a _cohort of children_—for example, a group of babies and the network of adults who care for them.
    - This **child-focused approach** naturally generates tight community bonds (teachers, parents, grandparents all have a stake in the children’s well-being).
3. **Local & Time-Bound**
    
    - A CCU is **local** in the sense that these 150 people physically or socially interact around caring for the same children.
    - It can be **time-bound**—e.g., lasting around 3–4 years—since children graduate or move on, and the CCU may shift membership accordingly.
4. **Narrow, Practical Scope**
    
    - By focusing on a _single, shared_ concern (raising children), members develop specific local knowledge, norms, and ways of solving problems.
    - This narrower scope **reinforces meaning** in areas like education, jobs, or social services, because everyone has direct, daily involvement.
5. **Basis for Community Analysis**
    
    - Because the CCU is small and **opt-in**, the group’s behavior and shared definitions (e.g., what counts as a “job,” “skill,” “support”) can be studied more accurately.
    - The **data** generated—on resources, local relationships, or conflict/resolution—is highly relevant for sociological or economic insights.
6. **Replication & Scaling**
    
    - The model envisions many such CCUs (each focusing on a different batch of children), rather than a single large system.
    - Each CCU can have **similar structures**—teachers, parents, childcare staff—but remain **locally unique** in how it defines and addresses community needs.

---

## **Why CCUs Matter**

- **Local Knowledge**: They capture the _real_ ways people discuss jobs, skills, and resources at a neighborhood or family level.
- **Coordinated Action**: Because everyone in a CCU shares responsibility for the children, it creates strong incentives to collaborate (e.g., job-sharing, skill exchanges, local events).
- **Research & Measurement**: A CCU offers a clear boundary for studying community dynamics—who’s in/out, how decisions get made, and how definitions evolve over time.

---

### **In Summary**

A **CCU** is a **child-centered community cluster** of about 150 people, designed as a **basic unit** for organizing and analyzing local social structures. Its size, focus on a cohort of children, and time-limited nature make it an ideal microcosm for understanding how real-world communities define and manage everyday issues like employment, education, and resource-sharing.



# 🔹 202502021006 - Tell us about yourself and your background.md 🔹
I am a sociologist who specialized in social network analysis. I dropped out to become a landlord - so I am ABD. I ran a stable business until covid and then I tried running a 'humanitarian' business, and got eaten alive. 

I believe that organizing knowledge locally is important. I have studied favelas, personal relationships and the cliquish nature of Notre Dame - I have had a variety of experiences dealing with knowledge, and my motivation is the preservation of unique knowledge. 

Mapping understanding means very clearly defining the channel and community you are speaking into. Reddit level. r/lansing_jobseekers , r/lansing_education.

Its because meanings drift over time - so the systems that capture them need to easily be able to define them.

Community
- manner of talking about Jobs
- manner of talking about education
etc

then you have the ability to compare communities of thought. This difference making helps the groups relate to one another, by acknowledging differences and difficulties.

How do you define a community though? objectively and subjective - both are wrong. This is because community and social organization arises from the behavior of the group. Abstractly the idea may exist, but until that abstraction is confirmed in the behavior of others, its not clear if it things are mutually understood. 

Therefore - it is easier to define community in an 'opt in' manner, with extremely limited ports of entry. 

The goal is maximum membership - but that is complex. what is something that all groups can opt into that they all mutually share the same understanding or believe they do.

really abstract things, sure, but the more local it becomes - the more difficult it is to have complete membership about anything. Because - specifics in relations matter, and most description is used to contrast. If the point of reference is not known- you can't make these assumptions.

So - therefore, creating a narrow context in which narrow use can be created, is the best way to reinforce meaning.

The DDC from the EEU will do that. Why isn't the DDC and the EEU the same - because EEU will never be big enough to serve everyone, though membership is unlimited. Because the way to create mutual benefit is to focus on a very  narrow scope and push.  Therefore - many EEEus will need to exist. I think one every 150 people would make sense - approximately the size of a CCU. 36 babies, 6 teachers, 72 parents and one grandparent each. 

Every cluster of 150 people focused on this or that batch of babies, is the basic organizing unit of people. 

In this context - the diversity of the parents and others becomes limited to the choices the parents will make about the children. from that narrow context - conflict and connection will be established, for a short period of about 4 years

My goal is to use this 'education units' as a basis of analysis, to measure and describe local communities.

So in my project, I am going to develop the tool necessary to capture this context. It will be modifyable - but it will provide the necessary base interlinkages to help concentrate and organize infromatino.

Babies - by their nature - are the product of their network of relationsihps. While they may be connected to abstract entities, these concepts do not have the same meaning. so their relationship to the world is much more direct. And therefore limited.

What we are going to be using a base unit of the CCU to evaluate and organize knowledge -Each baby in the CCU will have a larger context around them. Being able to measure and express that over time, will provide a tremendous amount of information back to the central cluster.

So my goal is to develop a system to establish the base community concepts of things like job and education. How does a community describe these things? How can this conception be reproduced automatically?

How can these things be captured, and how can they be used to describe a community?

' 

Hello - we are developing a local economic unit. We are measuring the local employment and skills sets.

Please in the field below - describe your current employment situation(s) and any skills that seem relevant. 
'

This data is heaped and mined - it is reduced to a different state and the request  language changes. 

Employment - types of employment, amount, 
process reduces it to 6 choices, and gathers specifics on the 6 parts.
Its an iterative process - questions constantly adjusting. 
Every sample begins  to describe the relationship to certain words people have.

From this data set - you train the model on sets of words and ideas - to createa corrispondence between these generic structures and these micro expressions

But you keep the base data as the source of language, and the coorispondance changes with time. 













# 🔹 202502020952.md 🔹
OK  I know what I want to reference.

With the DDc we are creating a community - a common context where people within a limited context can communicate. 

The graph is imagining knowledge as being this global thing - and it can't be, because all knowledge has a source and an agenda behind its awareness. So knowledge has to be organized by community. And what is great - is that the graph allows a community to define itself and contribute to the structure of knowledge, by actively participating. 





# 🔹 202502020951 - a **step-by-step example** of how you might approach creating and publishing a mini knowledge graph in GRC-20 format.md 🔹
Below is a **step-by-step example** of how you might approach creating and publishing a mini knowledge graph in GRC-20 format. We’ll pretend we’re collecting data on **local restaurants** in a city (e.g., San Francisco). You can follow this same structure for any other domain—jobs, education courses, events, etc.

---

# Example: Local Restaurants in San Francisco

We’ll go through the core steps: **(1) Pick the Domain & Scope, (2) Define the Ontology, (3) Gather & Clean Data, (4) Convert to GRC-20, (5) Publish & Document**.

---

## 1. Pick the Domain & Scope

**Domain**: Local restaurants in a city.

- **Example**: Restaurants in San Francisco, California.

**Scope**:

- We only want, say, top-rated restaurants (4+ stars on a rating site) or maybe a specific cuisine (Italian, Sushi, etc.).
- For simplicity, let’s gather a handful of restaurants (5–10) to illustrate the process. In a real hackathon project, you might scrape hundreds or thousands.

**Why This Matters**:

- We keep the data set small at first, so we understand the **mechanics** (defining the schema, GRC-20 publishing, etc.).
- Later, you can scale up to bigger data sets or more categories.

---

## 2. Define the Ontology

### 2.1. What Is an Ontology Here?

- An **ontology** in GRC-20 is basically how we describe the entity types (e.g. `Restaurant`) and the properties it has (like `name`, `address`, `cuisine`, etc.).
- We also define relationships, like how a `Restaurant` relates to a `City` entity.

### 2.2. Base Fields for a `Restaurant`

Let’s list the core properties we think every restaurant entry needs:

1. **Name** (string)
2. **Address** (string or structured object: `street, city, state, zip`)
3. **Cuisine** (string or from a standardized list like “Italian,” “Chinese,” “Mexican,” etc.)
4. **Average Rating** (number, e.g. 4.5)
5. **Number of Reviews** (number, e.g. 120 reviews)
6. **Price Range** (string or token like “$”, “$$”, “$$$”)
7. **Contact Info** (phone number, website URL)
8. **Operating Hours** (could be a more complex structure if we want day-by-day hours)

_(In a more advanced hackathon setting, we might define these properties in a GRC-20 “type definition” that says: “A Restaurant has these fields: …,” plus you might connect them to a separate `City` entity.)_

### 2.3. Relationship to `City` or `Region`

- We’ll store a reference to the **City** (San Francisco).
- If a GRC-20 entity for “San Francisco” already exists (published by you or someone else), we’ll just **link** to that entity ID.
- If it doesn’t exist yet, we might define a simpler `City` entity with fields like `cityName`, `stateName`, etc.

So the final shape for “Restaurant” could look like:

```yaml
Restaurant
 ├─ name: "Il Pastaio SF"
 ├─ address: "123 Market Street"
 ├─ city: [Reference to the 'San Francisco' entity]
 ├─ cuisine: "Italian"
 ├─ avgRating: 4.5
 ├─ reviewCount: 450
 ├─ priceRange: "$$"
 ├─ phone: "+1-415-999-9999"
 └─ website: "https://www.ilpastaiosf.com"
```

_(The exact technical format for GRC-20 may be JSON-based or a specialized on-chain format, but conceptually this is it.)_

---

## 3. Gather & Clean the Data

### 3.1. Identify Sources

- For a real project, you might scrape Yelp, TripAdvisor, or official city restaurant listings.
- For this example, say we found **5 Italian restaurants** from Yelp that have 4+ star ratings.

### 3.2. Clean & Standardize

- Remove duplicates. If the same restaurant is listed multiple times, unify it under one entry.
- Normalize the address formats (e.g., always `Street, City, State, Zip`).
- Convert rating data to a consistent scale (e.g., 1–5).

_(In a real scenario, you might use Python scripts or an AI-based tool to parse location info, detect duplicates by name/address similarity, etc.)_

### 3.3. Validate

- Double-check that each restaurant has essential fields (name, address, rating).
- Make sure any references to “City: San Francisco” match the official City entity ID.

---

## 4. Convert to GRC-20 (The Core Publishing Step)

### 4.1. GRC-20 Entity Format

GRC-20 is a standard for storing “knowledge tokens” or “entities” on-chain with typed properties.

- Typically, you’ll have **type definitions** (like `grc20:Restaurant`).
- Each actual restaurant is an **instance** of that type, with its own fields.

**Example**: You might have a JSON-like structure in code:

```json
{
  "id": "restaurant_il-pastaio-sf",  
  "type": "Restaurant",
  "name": "Il Pastaio SF",
  "address": "123 Market Street, San Francisco, CA 94103",
  "cuisine": "Italian",
  "avgRating": 4.5,
  "reviewCount": 450,
  "priceRange": "$$",
  "phone": "+1-415-999-9999",
  "website": "https://www.ilpastaiosf.com",
  "city": "city_san-francisco" // reference to city entity
}
```

_(This is just one possible format; the GRC-20 docs will show exactly how to wrap this for on-chain storage.)_

### 4.2. On-Chain Publishing

- **Method**: The hackathon might provide scripts or an API to publish GRC-20 entities.
- **Batch Upload**: If you have 5 restaurants, you might either mint them one-by-one or in a batched transaction.
- **Result**: You end up with 5 new “Restaurant” entities living on the chain, each with a unique ID.

### 4.3. Link to a “Public Space”

- In GRC-20, you might publish these to a “public space” for the city or for “Italian Restaurants SF.”
- That space has editors or members who can **vote** on changes (add new restaurants, update info, remove duplicates, etc.).

---

## 5. Document & Demonstrate

### 5.1. Documentation

- **Explain** how you got these 5 restaurants.
- **List** your final schema fields (like the bullet points above).
- **Highlight** any scripts or code used for scraping or cleaning.

### 5.2. Quick Demo or Queries

- Optionally, build a tiny front-end or run a GraphQL query (if provided by the hackathon’s tools) to show:
    - `query { Restaurant(where: { city: "city_san-francisco" }) { name avgRating priceRange } }`

_(This is just an example query—actual syntax depends on The Graph or GRC-20 indexing solutions.)_

- Show that the data is truly composable: if a separate team has published an entity for “San Francisco,” your restaurants are referencing it. Another dapp could now say, “Show me top restaurants in San Francisco,” and it’ll find your data.

---

# Putting It All Together

1. **Scope**: “We’re covering 5–10 top-rated Italian restaurants in San Francisco.”
2. **Define Ontology**: `Restaurant` with fields (`name, address, city, cuisine, rating, etc.`).
3. **Gather Data**: Use Yelp or TripAdvisor data, pick a subset of valid examples.
4. **Clean**: Ensure addresses and ratings are consistent.
5. **Publish**: Convert each restaurant to GRC-20 JSON (or a format the hackathon requires). Submit it on-chain under a public space, referencing the existing “San Francisco” entity.
6. **Document**: Provide a short readme or GitHub link with scripts, your data, and how it can be updated.

---

## Final Note

This is just one small-scale example. In the _real_ hackathon, you’d likely import:

- **Dozens, hundreds, or thousands** of restaurants or other domain entities.
- Use AI scripts to parse bigger data sets or standardize fields.
- Possibly collaborate with others so you’re not duplicating the same city or the same exact set of restaurants.

**But** the process is the same:

1. **Pick your slice of knowledge**
2. **Define the fields**
3. **Scrape and clean**
4. **Publish in GRC-20**
5. **Document & (optional) demonstrate**

That’s the essence of what they mean by “organizing knowledge via GRC-20.”



# 🔹 202502020949.md 🔹
[[202502020942- Introduction to the GRC 20 Hackathon]]
[[202502020947]]

Below is a **practical checklist** that covers what you need to organize, what the hackathon is asking for in concrete terms, and how to provide the results _successfully_.

---

## 1. **Clarify the Overall Objective**

### What’s Being Asked

- The hackathon wants teams to **organize real-world knowledge** (jobs, education, geographic regions, etc.) in a **Web3-native** format using the GRC-20 spec.
- In plain English, that means:
    1. **Define** or adopt a data schema/ontology (what fields do we store and how do we link them?).
    2. **Gather** relevant data (from Web2 or anywhere).
    3. **Convert** and publish it in GRC-20-compatible form.
    4. Possibly **demonstrate** that your data is now on-chain and structured well (so future apps can read it).

### What Success Looks Like

- A _high-quality_, _well-structured_, _usable_ dataset published using GRC-20 that other people or dapps can easily reference.
- Clear documentation of your process (which sources you used, how you cleaned/validated data, the steps to replicate).

---

## 2. **Pick a Domain (or Sub-Domain)**

### What You Need to Decide

1. **What category** are you focusing on? (Jobs, Education, Regions, or some niche domain you’re passionate about.)
2. **Narrow or Broad**? For example:
    - _Jobs:_ Maybe focus on remote Web3 jobs or just software engineering roles in general.
    - _Education:_ You could pick “Computer Science courses” or “Blockchain developer courses.”
    - _Regions:_ Maybe you focus on city-level data (restaurants, neighborhoods, events) in a specific set of countries.

### Why This Matters

- The hackathon organizers encourage not duplicating effort. If multiple teams do “all global cities,” you may be stepping on each other’s toes. It’s better to have a clear, unique chunk you’re responsible for.

---

## 3. **Define or Extend an Ontology**

### What is an Ontology Here?

- It’s a structured way of saying: “What _type_ of data do we store, and which _fields_ or _properties_ does each type have?”
    - Example (Jobs): `JobPosting` entity with properties like `title, description, requiredSkills, location, company, compensation`…
    - Example (Courses): `Course` entity with `title, institution, topicsCovered, skillLevel, linkToResource`…

### Steps to Decide

1. **Look at Existing Schemas**: GRC-20 will come with some recommended or example types. Familiarize yourself with them.
2. **Add Fields**: If your domain needs specialized fields (e.g., a “Doctor” job has `boardCertification`), you can extend the base schema.
3. **Stay Composable**: Don’t reinvent the wheel if a standard field type or property already exists. The point is to keep data interoperable for future apps.

---

## 4. **Gather & Clean the Data**

### What the Hackathon Expects

- **Real Data**: The end goal is to have _actual knowledge_ on-chain, not just a theoretical schema.
- **Sources**: Could be websites, open APIs, data sets, or even manual curation.

### Action Items

4. **Identify** your sources (e.g. “We will scrape 5 major job boards and cross-check them.”).
5. **Decide** how you’ll automate (scrapers, scripts, or AI text extraction).
6. **Clean** up duplicates, fix inconsistencies, unify references (e.g., “San Francisco” vs. “SF”).
7. **Verify** the data for correctness if possible. (Governance is a big theme; you might not fully solve it, but at least aim for minimal error.)

---

## 5. **Publish the Knowledge via GRC-20**

### The Core Delivery

- After your data is well-structured, you’ll **publish** it in the GRC-20 format on the designated chain (likely the “Geo Genesis” or a test environment they provide).

### How You’ll Provide It

8. **Convert** each data record to the GRC-20 “token” or “entity” format (the exact steps should be in the documentation or templates they give).
9. **Push** it on-chain. This might be done by:
    - Using a provided script or library,
    - Deploying your own contract that mints or updates GRC-20 entities,
    - Or an upload interface if they offer one during the hackathon.

---

## 6. **Document the Process & Demonstrate**

### For the Hackathon Submission

- You’ll likely need to provide:
    1. **A short write-up** on _what domain you chose_, _why it’s valuable_, _where you got the data_, and _how you validated it_.
    2. **Technical instructions** on how to reproduce or extend your scripts.
    3. Possibly **a basic UI or test queries** to show the data is truly stored in a structured manner and can be read by others.

### Why Documentation Matters

- GRC-20 is brand new. Judges and the community will want to see _how you approached it_ and how to keep building on it.
- The easiest way to stand out is to have clarity and a step-by-step approach that others can follow or verify.

---

## 7. **Plan for Ongoing Curation or Governance**

### Optional but Encouraged

- Since this is a _public knowledge graph_, your data might evolve. For instance, job openings expire or location data changes over time.
- **How** might you or others keep it up-to-date?
- If you create a public “space” (editors, voters) under the GRC-20 governance model, be clear on who is in charge of approving future edits.

---

## 8. **Putting it All Together: Step-by-Step Example**

Below is an example process flow to keep things organized:

10. **Decide Your Domain**
    
    - E.g., “US City Data: Basic city details + major neighborhood info + top local events.”
11. **Check or Draft an Ontology**
    
    - Look at existing region/city schemas if provided. Extend them with your own fields (e.g., `population`, `timeZone`, `majorAttractions`…).
12. **Identify Data Sources & Scraping Plan**
    
    - Maybe you use public Wikipedia data or city-data.com.
    - Plan a script or AI extraction tool to gather consistent fields.
13. **Scrape and Clean**
    
    - For each city, standardize names. Remove duplicates or incomplete entries.
    - Make sure location referencing is consistent (e.g., city → state → country).
14. **Format into GRC-20**
    
    - For each city, create the on-chain entity with the fields.
    - Ensure it references official IDs or parent entities (like the “State” or “Country” entity ID).
15. **Publish & Document**
    
    - Put your data on the chain.
    - Provide a GitHub repo or doc with your entire process, your scripts, and info about how to read/verify the data.
    - Possibly show a small front end or query tool as proof-of-concept.
16. **Explain Next Steps**
    
    - Indicate how more cities can be added, or how an editor can correct an entry if the population changes, etc.

---

## 9. **Things You Need to Tell Your Team / Organizer**

- **Your Domain**: Exactly which slice of knowledge are you going to tackle?
- **Your Data Sources**: Where are you pulling from, and are they open?
- **Your Quality Strategy**: Quick mention of how you plan to validate or remove bad data.
- **Your Ontology**: Which fields will you store and how does that tie in with GRC-20?
- **Your Tools**: Are you using Python scripts, custom scrapers, AI APIs, etc.?
- **Your Publishing Method**: How you’ll mint or push the data on-chain (the specific GRC-20 approach).
- **Your Documentation**: A place everyone can see your final structure, code, and instructions (GitHub, Hackathon page).

---

## 10. **Key Takeaways**

17. **It’s about data + structure**, not just building a UI.
18. You need a **clear scope**: pick exactly the knowledge domain you want to handle.
19. **Define an ontology** that fits GRC-20 but can be extended if you have special fields.
20. **Scrape or gather** as much relevant data as possible, then **clean** it.
21. **Publish** in GRC-20 format, show it off with minimal friction for judging.
22. Provide **clear documentation** so others see how to keep it updated, fix errors, or build apps on top of it.

---

### In Short

You’re being asked to **organize** some slice of real-world data in a composable, on-chain format. To do it successfully:

23. Be clear on _which data you want_ (domain).
24. Develop a _schema/ontology_ or use the recommended GRC-20 approach.
25. Write scripts/AI tools to _gather and clean_ data.
26. _Publish_ it on the chain with a solid explanation of how future developers can use or extend it.

That’s precisely what the hackathon is about, and those steps outline how you’ll **provide the finished results** in a way that meets the judges’ (and the broader community’s) expectations.



# 🔹 202502020947.md 🔹
[[202502020942- Introduction to the GRC 20 Hackathon]]

Below is a distilled explanation of what **the GRC20 Hackathon** is all about, why it’s happening, what the participants are actually doing, and how it all fits together. The conversation can feel a bit “rambling” because they’re covering a lot of conceptual ground—so here’s the concise breakdown:

---

## 1. **Hackathon Purpose and Big Picture**

- **Overall Goal**: To organize real-world knowledge in a “Web3-native” way, using a new spec called **GRC-20**. This specification is meant to solve the problem of how to store and interlink data/knowledge so that it’s **composable** (i.e., easily referenced and extended by many apps) and **verifiable** on-chain.
- **Why It Matters**: Today, most decentralized apps (dapps) focus on financial data (tokens, NFTs). Non-financial data—like job listings, local info, education content—is mostly stuck in Web2. GRC-20 is about bringing _all kinds_ of knowledge on-chain with a standard that multiple apps can reuse.

---

## 2. **What Is Being Built During the Hackathon?**

- **Not “Just Another App”**  
    Normally, hackathons involve building an app from scratch. Here, the main focus is on **organizing and ingesting data** into a newly created on-chain knowledge graph using GRC-20 standards.
    
- **Focus Areas** (they picked three big ones):
    
    1. **Jobs**: Example: building a standard, composable set of “Job” data—roles, skills, descriptions, salaries—so that future dapps can easily share and filter these listings.
    2. **Education**: Example: structuring academic fields, courses, lessons, training resources, so that future “web3 universities” or job sites can reference them.
    3. **Regions**: Example: standardizing location data (cities, neighborhoods, countries) so that events, local businesses, or any location-based info is all cross-compatible.
- **Concrete Work**:
    
    1. **Define Ontologies**: This means deciding how the data is structured. For instance, how do we define a “Job Opening?” Which fields are mandatory? How do we define “City?”—which attributes do we store?
    2. **Scrape / Import Data**: Teams might write scripts or use AI to pull existing data from Web2 sources (like Indeed, LinkedIn, event sites, or city data), and ingest it into the new on-chain knowledge graph.
    3. **Clean & Curate**: Because the data is supposed to be _accurate_ (or at least well-organized), part of the job is to remove duplicates, keep it high-quality, and ensure it’s valid for Web3 usage.
    4. **Publish** the knowledge in GRC-20 format on the chain.

---

## 3. **How the Hackathon Is Structured**

- **Duration**: 5 weeks.
- **Prize Pool**: 150,000 GRT (The Graph’s token) as rewards for the best contributions.
- **Week 1**: Teams and individuals figure out exactly _which chunk of knowledge_ they want to import. They coordinate so there isn’t a ton of overlap (e.g., you don’t want five teams doing the _exact_ same job-listing scrape).
- **Following Weeks**: You build scripts or use AI to gather data, standardize it according to the GRC-20 approach, and push it to the chain.

---

## 4. **Why GRC-20?**

- **Analogy**: Similar to how ERC-20 standardized how tokens work, GRC-20 aims to standardize how knowledge/metadata is structured so it’s easy for multiple dapps to read, reference, or expand upon it.
- **Composability**: If a job listing is minted with GRC-20 standards, _any_ future dapp can connect to that data, display it, or update it, without building from scratch.
- **Governance**: Public data has to be curated. GRC-20 includes ideas for how to have “spaces” with editors and members that approve changes, track versions, and keep quality high.

---

## 5. **The Role of AI**

They discuss AI in two ways:

1. **Data Ingestion**: Instead of manually typing in thousands of lines of data, you can script AI tools (or do large-scale web scraping + AI) to extract, clean, and structure big sets of real-world knowledge.
2. **Retrieval & Consumption**: Once the knowledge is on-chain in a structured way, you can query it with large language models (LLMs) and get _accurate answers_ (because it references an up-to-date, on-chain knowledge graph rather than random old data).

---

## 6. **What’s Expected of Hackathon Participants?**

1. **Pick a Domain** (jobs, education, regions, or something similar).
2. **Plan** how you’ll gather relevant data (scraping, open databases, crowdsourcing, etc.).
3. **Define** or extend an existing GRC-20 “schema”/“ontology” so the data is standardized.
4. **Write Tools or Scripts** to import that data onto the chain, ensuring it meets quality standards (no duplicates, consistent definitions).
5. **Document** how it’s done (so others can build on or maintain it).

You don’t have to build a fancy front-end app right now (that’s _later_, once the knowledge is on-chain). However, any smaller UI or demos that show your data is properly structured might help with judging.

---

## 7. **How to Get Involved**

- **Apply**: They mentioned an application link (likely on The Graph’s official channels or Geo’s announcements). It’s short and mostly just ensures you’re a real person interested in the theme.
- **Join the Discord**: The Graph’s Discord is where participants coordinate, ask questions, and get assigned or self-select topics.
- **Form or Join Teams**: If you’re more of a data person or a developer, find others who complement your skill set (e.g., AI folks, scrapers, or domain experts in education, etc.).

---

## 8. **Key Takeaways (in Plain English)**

6. They want to **pull real-world knowledge** into a shared “Web3 data layer” so future decentralized apps can all share the same _accurate, open_ info.
7. **GRC-20** is like a universal schema for on-chain data. They’re using it to store things like job openings, courses, or city info in a consistent format.
8. **Your job** in this hackathon is basically to help decide how data is structured and then _bulk-import_ or _curate_ that data. You can do it for any domain—though they suggested Jobs, Education, or Regions to get the ball rolling.
9. Everything is **open** and eventually will help “web3” go beyond purely financial use-cases, enabling _composable_ knowledge-based apps (e.g., better event apps, local directories, job boards, etc.).

---

### In Short

- **They are not building one specific product**; they are building the **foundation** for many future products.
- You’ll spend your hackathon time **finding, cleaning, and publishing** relevant knowledge in a GRC-20-compatible way.
- The idea is that once we have well-structured knowledge on-chain, we can build truly composable, unstoppable decentralized applications around it.

That’s the core message—hope it helps clear up the “why” and “what” of the GRC20 Hackathon!



# 🔹 202502020942- Introduction to the GRC 20 Hackathon.md 🔹
https://www.youtube.com/watch?v=8P-FEjYGI0Y

## Introduction to the GRC 20 Hackathon

0:00

all right GM GM hi everyone how's it going the graph Discord how is it going

0:06

graph devs we are streaming on graph devs and we are in chat in the graph

0:13

Discord and welcome to another episode of the graph Builders office hours and we've got a amazing guest again so he's

0:20

coming through I think you were here what a month or month and a half ago for the GRC 20 spec and you're back for the

0:27

GRC 20 hackathon we've got your knees tall every one how you doing you knes hey Marcus it's good to be back it was a

0:33

lot of fun last time and uh I'm excited for this one yeah this is this is I feel

0:40

a good representation of how Tech at the bleeding edge of tech is

0:49

best produced where we have a new spec getting feedback getting awareness to it which was a few weeks ago and now we're

0:55

like hey let's go use it and let's go push it into the public let's go ahead and see what people can actually move

1:01

forward with now the more I'm not going to use the word finalize but I'll use the word it's more well-shaped so really

1:08

exciting times hi everyone in chat how's it going uh we're going to wait probably two maybe three more minutes for people

1:15

to filter in I'm looking at chat right here just so everyone knows and uh in the meantime Y how is your day going so

1:23

far and how is preparing for this gc20 hackathon going yeah things are going

1:29

great I'm super excited exced for it uh we just announced it on Monday and with

1:34

basically one week for people to sign up so it's moving quickly but I'm glad that you know we do these uh you know Builder

1:41

office hours every week so had a chance to kind of jump on but yeah we're getting things ready we're ready to kick

1:46

things off it's coming right up yeah it is really coming right up I'm dropping the links in the chat excuse me they are

1:52

in Discord and they are going excuse me they're going to be on Twitter as well and so from there getting the word out

1:59

about this hack of and how's it going chat if there's any questions about this hackathons feel

2:04

free to Shir the questions there we'll be doing a Q&A at the end and in terms of announcements that we have going on

2:11

right now really the biggest announcement that I want to push is in fact this hackathon and for anyone out

2:16

there who is wanting to get into the Q&A in this this the Q&A that I'm looking at is in the graph Discord so go ahead and

2:23

jump into the graph Discord we do the graph Builder office hours every Thursday at 5:00 p.m. UTC and we feature

2:29

either core development teams within the graph ecosystem or we also feature Builders from the outside the ecosystem

## The Vision of Web3 and Knowledge Graphs

2:35

such as pains such as daps and we make sure that they get their shine because it's really important that developers of

2:42

all backgrounds all builds all come in here and share what they're building that's a really the Mind share is really a powerful setting scpt so any thoughts

2:49

that you want to share you need besides just getting right into the workshop happy to get started awesome awesome

2:56

okay so I'm going to step off stage I'll let you get started and chat like I said if there's any questions drop them in

3:01

and have a good presentation thank you I'm going to start sharing and let

3:09

me know if this does or doesn't work hopefully that's good so hey everyone it's great to be back with you and today

3:16

we're going to be talking about the GRC 20 hackathon uh that we announced on Monday it's going to be starting in just

3:23

a few days uh this upcoming Monday um so Post in the chat I can't actually

3:29

necessarily see right now but Post in the chat if you've been following the graph for at least two

3:35

years um lot's been happening over the last two years it's been a really fun time and post in the chat if you've been

3:43

following the graph for over four years so around the the network launch I'd be curious how many people

3:50

have been around since then um so for for those that have been using the graph and following the graph for a while you

3:57

know we've been working towards this vision of web 3 for a very long time really since

4:04

2017 and um and we're really excited we're going to be pushing this thing forward to the next stage you know it's

4:11

a really big Vision this whole web 3 idea and you know sometimes you have to take it just uh you know one step at a

4:18

time um so you know people that have been around for a while will remember that before we launched the graph

4:24

Network we did a hackathon called the Space Race um and uh there's going to be some commonality between this and the

4:30

Space Race we also did this kind of curator program but uh but you know this

4:35

is going to be a really big step in realizing the vision of web 3 um uh

4:42

people have been posting about the haathon on X which I really appreciate keep it up um I like this one from this

4:50

uh crypto commentator right this is the start of how we take all of the world's knowledge and bring it on chain

4:57

historic um so I agree I think is going to be historic um I want to start with

5:03

uh just a very quick overview uh of why knowledge graphs and and what is web 3

5:11

you know web 3 is in crypto is really a crazy industry right because we keep

5:16

cycling between these like Manas where we're like you know web 3 is going to change everything right it's this

5:23

revolution uh to to the other side where it's oh you know maybe it's not actually

5:28

going to be used for that much maybe it's just just you know bitcoin's a store of value and that's it or

5:34

something like that and the reality is I do think web 3 is going to change

## Organizing Knowledge for Web3 Applications

5:41

everything uh but we just needed the right pieces and um and and and I

5:48

believe we believe that knowledge graphs are really the missing piece right

5:53

because what is web 3 it's a new platform for decentralized applications

5:59

and specific speically it's a new web that's verifiable open and

6:04

composable right and you know with decentralized networks in crypto you know people kind of understand

6:10

verifiability and openness but what I think we've really been missing is this composability and knowledge graphs are

6:17

really the solution to composability right we want to create an a web where

6:23

the information is verifiable it's open and you can use it across applications

6:29

um and you know we start to see a little bit of that in some places in web 3 but you know to really unlock the real thing

6:36

we need like the right Primitives and uh GRC 20 is that primitive for composable

6:43

knowledge and information across applications um so uh we're going to

6:48

start uh hacking on that uh with this hackathon and this will be you know a

6:54

really great chance to learn a ton are you're going to learn about knowledge graphs learn about AI uh if you haven't

7:01

had a chance to play around with AI much uh and and especially if you have but in either way this will be a really great

7:08

way to you know brush up your chops learn from other people and help to bootstrap uh the next generation of web3

7:16

applications um so if you know we view web 3 as being right this open

7:22

verifiable composable web uh we want to bootstrap that we want to have organized

7:29

knowledge and information that people can use for their applications and you know this was really you know always a

7:35

big goal with the graph right is to have open data and open knowledge and information um that makes it easier for

7:42

people to build amazing applications um and uh uh you know for that you know the

7:48

high level goal is really just to have more useful knowledge and information better organized right the the the more

7:55

useful the knowledge is the the more applicable it is to a larger number of people the more people will benefit from

8:02

web 3 the more applications we can build on web 3 so that's really kind of this highlevel goal uh and as far as you know

8:10

developers you can really think of two different sides of this kind of web 3

8:15

organized graph equation right there's um organizing the knowledge that's kind of half of it right making sure that

## Focus Areas: Jobs, Education, and Regions

8:22

it's well organized and it's it's easy to use um and then the other side is

8:29

building the applications right um building web 3 apps so for this

8:34

hackathon we're going to focus on the first half so this is kind of the first big step right um organizing some

8:41

initial knowledge um that uh is using this gc20 primitive it's composable it's

8:48

extensible it's all these things that we want web 3 to be um and then on the other side uh as a next step down the

8:55

road uh coming up soon uh we will be able to build incred inredible web 3

9:00

apps that are way sicker than any apps that people have seen before in web 3 uh and it's going to be enabled by these

9:06

New Primitives um so the hackathon is five weeks it starts on February 3rd it's

9:13

online you can join from anywhere and we've got 150,000 GRT up for grabs um in

9:21

bounties uh and uh let's see um I guess

9:27

I'll just jump in straight into uh some of the uh areas that we want to focus on so you know our goal is to organize you

9:35

know the world's public knowledge and information um you know uh there are

9:40

commonalities between that mission and Google's mission right theirs is was also you know to to organize and make

9:46

Universal useful all the world's you know knowledge and information so you know people call called the graph the

9:51

Google a blockchain um but we need to get the real world data and knowledge on

9:57

chain on web 3 right and so uh we think a fun way to do this is to to bring in

10:04

some Focus areas so that people can collaborate and work together on a few kind of focus areas and and uh and we

10:11

think that'll be a really fun way to do this you can learn from each other and we can build something that's really useful in a few areas um and and we want

10:19

web 3 to move Beyond just kind of speculation beyond the the casinos um

10:24

you know uh and and so you know web 3 really is going to uh you know change

10:31

the whole economy right we we we have you know there's we have an opportunity to have you know uh a golden era right

10:38

now right a new Golden Age The Roaring 20s um and uh and web 3 is going to be a big

10:46

part of that so we need to get out into the real world so three of the areas we want to focus on are education jobs and

10:54

regions and um and let's start uh for example

11:00

with uh with jobs so um I want to

11:07

show um some brief examples so uh you may

11:13

have seen that we've launched geog Genesis it for Early Access um we're

11:19

we're going to be opening up opening this up more and more you know over the coming days and weeks um but uh we're

## The Role of AI in Knowledge Graphs

11:27

already starting to organize knowledge for uh uh a few different Industries

11:32

focusing on crypto to start um now this goo is just one application right um on

11:41

top of knowledge graphs you could build any kind of application right so so this is just you know a way to kind of see

11:48

knowledge and information in one way and and to contribute to it in one way um you could think of this almost like a

11:54

maybe like a a CMS or something right if you got your like WordPress and you've got a place you can go in and you can see all your stuff and then if you want

12:01

to have a custom website or something like that to your storefront you can right so um so here for example for the

12:08

crypto industry we'd want to see things like news you know maybe upcoming

12:14

events um you know projects that are in the crypto space um people who's working

12:20

in this industry uh and jobs and with jobs you'd want to see things like you know what are the different roles you

12:27

know maybe you know there's software engineering uh what are the different skills maybe solidity is super In Demand right maybe

12:33

rust is super in demand or or other things like marketing um so for every

12:38

industry you want to kind of organize these sorts of things um for the people that are working in that industry um and

12:46

uh and people who want to break into the industry um and uh and so these are just

12:52

kind of a few examples um now this is an older version of geog Genesis this is

12:58

what we were uh using before we launched um the uh

13:05

the new version that's based on GRC 20 uh we still have a lot of content here that we want to migrate over uh but uh

13:12

for now it's here uh and um just to show a few examples of a few different kind

13:17

of Industries um now uh people might have different industries that they're interested in and that's really great

13:24

and this is where we can kind of divide and conquer and divvy things up um personally I think agriculture is uh a

13:32

really interesting industry I think we want to see really big changes you know in places like the US where you know

13:39

they're using a lot of chemicals and unsustainable practices you know sometimes you know poisoning our food um

13:46

and you know I would love to see a shift you know towards more sustainable and uh

13:51

healthier uh practices for for agriculture so you can imagine that's an industry and you know at the same time

13:57

there's realities of that industry they you know they need to produce you know fruits vegetables and Grains and

14:03

livestock and all of these things um and the economics might be difficult you know there's lots of knowledge that's

14:10

required to do that job well um and so this is just an example of how to kind of you know organize an industry around

14:17

goals and try to see even you know from a top level like you know what are the areas of the economy that we'd love to

14:23

see transformed uh and maybe web 3 can help uh if we look at areas like social

14:29

work uh this is related to some of the work that we've done but here we've done I think a a better job it's still just a

## Hackathon Structure and Goals

14:34

beginning you know we have the kind of the goals but we also have uh here like different roles and skills so you could

14:43

imagine that if you're doing something like a you know let's say like a web 3 LinkedIn you know one of the first

14:49

things you do right is you create your resume and you're adding your skills and so what you want is a sort of drop down

14:56

of like here are the different roles here are the different skills and you want you want it all to be

15:02

interconnected right so that if I'm looking for a job and the job is you

15:07

know to be a software engineer or maybe it's a specific type of cryptographer or something that I could learn about that

15:13

role see you know which other uh companies are hiring for that same role

15:19

um and and maybe I can like learn about uh you know how How would how would I be

15:25

able to uh to to uh apply for something like that like what courses could I take

15:30

or what could I learn um and that takes us to the second area right which is

15:37

education um you know it's especially in the US you know um you know I think uh a

15:45

lot of people are seeing that the university system and even you know um you know grade schools you know that

15:51

system it's it's it's pretty broken and uh you know higher education Higher

15:57

Learning is is is really valuable but you know how how how do we organize the

16:03

accreditation how do we make it more accessible to people how do we make sure that you know these things aren't

16:09

getting captured um and and stop evolving right A lot of the stuff has

16:15

become really politicized and so you know I think web 3 uh can really uh play a big role in

16:23

helping to you know revolutionize education making it more accessible higher quality

16:29

um and uh and and all you know so much of that is knowledge you know um and

16:36

so to start you know we could look at what are the different you know academic

16:42

fields and um and for each of the academic Fields like what are the different courses what are the different

16:49

lessons what are the different topics so and and we could see you know different

16:54

degrees uh things that you know Pathways that people can learn accreditation things and and then we

17:01

could tie that to the jobs we've got some examples of you know maybe for

17:06

example like cryptography you know it's also related to um you know crypto and

17:13

maybe you could actually learn cryptography on something like this um so you could start just by like C

17:18

categorizing and kind of organizing these top level topics lessons courses this kind of stuff if you know get to

17:26

more and more content uh but just the organization of it and the mapping it um

17:32

is uh is really powerful um uh and then finally uh

## Engaging with the Community and Q&A

17:39

regions so uh you know we live in the real world and uh and we have you know

17:47

uh existing countries and states and cities and we want all of that to be part of the knowledge graph right this

17:53

is really going to empower Next Generation applications so you know with the Knowledge Graph we want to have it

18:00

so that you know people can search for things and there's already an entity for

18:06

that thing right so um let's say that I was building an app for events right I

18:11

want to build the next Meetup and somebody's creating an event and it's going to be in you know the city of San

18:19

Francisco uh what do we know about the city of San Francisco is it something that you can point to and and then you

18:25

could kind of query to for example show all of the events that are happening in San Francisco that would be really great

18:32

um but you know if every application is using a different ID for the city of San

18:38

Francisco then how are we going to do that right these guys are using this ID those guys are using that ID and then

18:43

there's no way for me to aggregate that really well together and so that's where the composability comes in and that's

18:49

why we really want to have this like Global Knowledge Graph and so I would want to for example search for San

18:55

Francisco and see oh here's a city right the city of San

19:00

Francisco and I could integrate that into my app right so you know you could have autocomplete for everything uh and

19:09

we'd want to have information about like cities so for example here right the city of San Francisco maybe you'd want

19:14

to have like you know local news and events and places right at all the local

19:19

businesses you know and we can do things like arts and culture you know and and help Revitalize cities and help people

19:26

find interesting things to do um so uh these are all examples of the kinds of

19:32

things that we could do at a local level right and we we could do this at a local level right for cities um but also we

19:38

can organize information that's relevant to like countries or states or at different um you know kind of Scopes so

19:47

this is just an example right of what a city page could look like with Upcoming Events maybe trending restaurants

19:53

neighborhoods and all of this stuff would then be part of the knowledge graph so you know the more knowledge

19:59

that we add the better it's organized right and now using this kind of composable

20:04

ontology um where you know you can have the ReUse and you can have the composability but you can also customize

20:11

things right this is really what web 3 is all about um you know we want to make it easier and easier to build the next

20:18

great app um and when we have these composable Primitives that's exactly

20:25

what we can do um so so this is some example of some of the content that we

20:30

could look to um to organize uh for those three different areas right the jobs the education and

20:38

the regions and uh next I just want to talk a little bit about um you know some of

20:45

the tools that you could be using and uh um and and you know some skills that you

20:51

could learn along the way so we've talked a little bit about knowledge graphs and um you you know right now uh

21:00

AI is super hot right hopefully you've had a chance to play around with AI um

21:05

it's really incredible what we can do with these tools um so AI is really going to intersect with knowledge graphs

21:11

in two main ways one is it's going to make it way faster and easier to ingest

21:18

knowledge and put it into the knowledge graph right AI can automate a lot of stuff that before would have taken us a

21:25

lot of time so it's really a superpower in that way you can leverage those superpowers to do super things um the

21:32

other way that uh the two really intersect is in how you consume the

21:38

knowledge from the knowledge graph are these llms are super powerful and you

21:43

you want to be able to uh have a great easy interface for you know uh

21:48

retrieving knowledge with natural language and so uh there's really you

21:54

know two different ways right now that people kind of structure this stuff for llm so uh you have these embeddings and

22:00

vectors right which deals with the textural representation of the

22:05

information and uh some of the things that llms uh you know some of those components do really well is you know if

22:12

you have these embeddings um you can do kind of similarity searches right and and those things are are really great um

22:19

you know if you have some question it it can find like relevant information based

22:26

on similarity of of the word words um and that's really powerful uh but what

22:32

that doesn't do is actually encode you know real meaning right it's this kind

22:38

of probabilistic thing but it doesn't actually know what is correct whereas with knowledge graphs

22:46

you're able to structure things you know really accurately the relationships between different things and you're able

22:53

to keep that up to date right so we've all had the experience of you're using an llm and gives you an answer that is

22:59

suboptimal right maybe it's maybe it's in you know it maybe it was true you

23:05

know a year ago but it's no longer accurate or you know there's all kinds of ways in which the in information that

23:12

it gives you might might be wrong and then it's it's hard to kind of correct it and it's hard to know okay you know

23:18

what exactly did you base that answer on now some of these AI tools have gotten better because they can actually link to

23:25

the primary sources but again it's not not very precise and so in an Ideal World we

23:32

would have entities for everything everything would be structured correctly the entities themselves and the

23:38

relationships between them and then when you ask an Lama

23:44

question what it does is a graph query against the knowledge graph right so you

23:50

ask a question right it does a graph query against the knowledge graph gives

23:55

you back the relevant information and then the llm can take that and generate the answer that's based on the correct

24:01

accurate rep you know knowledge um and so that is extremely powerful um and uh

24:10

and so that's kind of some of the stuff that we'll be able to do you know when we structure more of this knowledge and

24:16

information into a Knowledge Graph um for this

24:22

hackathon um you know using AI to to ingest a lot you know this knowledge and

24:30

information it's going to be a real superpower and you'll be able to play with different models and uh and and

24:36

build up your stack of tools for leveraging AI uh to to do cool things um

24:45

so uh that's some of the content I wanted to cover so I'd be super interested in just kind of opening this

24:51

up and kind of having a conversation um about uh you know the hackathon about

24:57

knowled graphs about what we want to accomplish really open to any kind of questions and conversations at this

25:03

point awesome awesome and so yeah we do have some good questions in the chat

25:10

we'll go right down the list here we go any idea who will be the judges if yes

25:18

can you share um I don't know that I want to share the specific people per se but um It's a combination of folks at

25:25

the graph foundation and at Gio all right next question can you give a couple examples of what you would like

25:31

to see for this hackathon proposal yeah now keep in mind for so okay the

25:37

submission is meant to be pretty simple so um so there's a link here right to

25:42

apply uh we just have a few questions and um and honestly I mean you you

25:48

really just want to demonstrate that you're a real person you know and that uh this is an area that you're

25:53

interested in um but uh you know don't be too worried about the application as

26:00

long as uh those things apply uh now we're going to spend the

26:05

first week of the hackathon basically um you know coming up with

26:12

ideas and focus areas and uh when it comes to this type of a hackathon

26:17

there's kind of an interesting balance that we want to strike it's interesting because we're actually going to create

26:22

something that is is useful right A lot of times with hackathons uh you you do a project and

26:28

then it's kind of throw away right our goal is that you know you can hack hack

26:34

around and you can learn and experiment and play and and do some cool things but the thing that you build is actually

26:40

going to be useful right maybe you could enable like you know 10 amazing applications based on this so so that's

26:46

really cool um and uh you know so to do that essentially what we want to do is

26:54

balance like what kind of knowledge and information would be most use useful and

27:00

we have some ideas about that and you know maybe you also have Amazing Ideas maybe your ideas are better than ours so

27:06

you know we want to you know try to look at okay what would be the most useful knowledge and information that would

27:11

enable interesting applications you know in these areas um and then we want to match that against really your interests

27:18

right what what things excite you you know you'll you'll always do better work if uh if you're naturally passionate or

27:24

interested in a specific you know in the thing that you're doing so that's what we got to figure out is what does that

27:30

match and in an ideal world uh we we minimize duplication you know it would

27:36

be unfortunate if you know two different people or teams you know spent four weeks they came back and they had the

27:43

exact same knowledge that they were importing right um that would be duplicated uh and and potentially wasted

27:49

work so in an Ideal World what we do is we kind of figure that out and then we

27:55

kind of assign areas of ownership that based on on what would be useful and based on what you're interested in and

28:01

then you kind of know okay if I knock this out of the park and I spend a bunch of time and I do a good job with this

28:06

you know it's it's it's not going to be duplicated it's going to really become part of this Global graph um that could

28:13

enable a bunch of interesting applications yeah very cool and uh I

28:18

will second that notion about haathon teams build for yes the hackathon and

28:25

then also likely if you're thinking big and you're thinking in terms of what

28:32

could be useful in an interesting project to you long term not only are you more likely to win the hackathon but

28:39

also uh carrying the project on and forward and really internalizing a

28:45

different way of thinking is the long-term benefit so yeah for sure go

28:50

for the short-term prize that's great and also think about something that you can carry with you and that that's

28:57

that's second that you need was a good concept there see a little bit of a longer

29:03

question here what is your take on the GRC 20/ geog Genesis tackling the parent

29:10

child cousin structure of l1's versus l2s versus cross chain perah subhub

29:16

chains living in l2s and l3s Etc I would love to share my take maybe as a

29:21

hackathon proposal but not sure it would even a h so that's super interesting

29:29

um would love to hear your take um I do wonder if what you're thinking of

29:35

building might be different than this goal of organizing knowledge uh for this

29:40

hackathon um so maybe that becomes an interesting one for a a different hackathon that we do but for sure the

29:46

infrastructure stuff is interesting so um yeah I would love to chat about that I mean right now geog Genesis is

29:53

launched on uh one chain right that's it's it's a it's technically a layer

29:59

three chain on top of arbitrum and uh and that's just kind of the initial chain one of the reason you

30:05

know it's it's geog Genesis this is just kind of the first step but ultimately knowledge is going to live on lots of

30:11

different chains right no question about that and so uh you know all of this

30:17

infrastructure you know should be open and composable um now the considerations

30:23

with knowledge is a little bit different than the considerations with value and it's interesting to to consider the

30:28

blockchain engineering implications of that right um when you're talking about value and money um your primary concern

30:36

is double spend right and what you don't want is you know somebody to move a million dollars onto a chain and then

30:45

that money gets stuck or it gets stolen by The Operators and then and then you can't get it back right so so the that's

30:52

the type of security you're thinking about for money now for knowledge

## The Nature of Knowledge and Data Availability

30:58

knowled doesn't exactly have that property right it doesn't necessarily like get stuck right it's not like ah my knowledge is over there and now

31:04

I can't use it right uh and so you do want the data to be available right you want data availability you want you know

31:11

good Storage Solutions and accessibility you want it to be index so you can query it but but there is value in having

31:19

consensus We Believe on knowledge to kind of you know uh for the same reason the databases are acid compliant right

31:27

and asset compliant database has like automic and it has like

31:32

consistency um and for many Information Systems you want that consistency you don't want it so that you know things

31:39

are in a inconsistent State and um and so so these are kind of interesting

31:46

questions so ultimately you know uh the question of how do you create a global graph where there's uh you know

31:52

consensus on the state of the knowledge graph even as you know people are

31:58

applications that are living on different chains and all this kind of stuff that's super interesting and uh

32:03

yeah we should be having more you know uh discussions about those types of topics yeah and I I might I see a few of

32:11

questions on there but this is a followup for myself to you you need and I know this is geog Genesis I know this

32:17

is gc20 that literally just a few weeks ago was proposed publicly and now is

32:25

taking more stape and having testing and I mean this is this is a big push what

32:30

would you say to any other developers or BD or marketing or anyone tuning in

32:37

right now who's part of another protocol who is just starting to hear about GRC 20 and what it might mean for them uh in

32:44

the future yeah so at this stage you know when things are early you know it's it's

32:52

probably best to be part of like you know the the core crew of developers that are building it develop you know it

32:58

developing it because that's how you're going to learn the most you know we want to shape this as a community so you you

33:04

want to get involved right so things like this hackathon is a great way to get close to that conversation now uh

33:09

what's it going to mean as we as this stuff grows right and so you know I think this hackathon is going to be a

33:16

really big piece of demonstrating the next level of growth and then we'll have

33:21

a lot more that we can see we'll have a lot more developers that understand how to use these tools and they can go off

33:27

and they can teach other people so uh you know at that stage maybe uh you know

33:33

we'll be ready for people to use us in different types of applications you know keeping in mind also um you know right

33:39

now we're we're kind of organizing the knowledge on on Geo here we're going to be using tools outside of Goo right

33:46

that's why you know it's a hackathon so developers will be writing scripts and tools and different things to generate the

33:51

knowledge uh but we haven't yet released the tools to build custom applications

33:57

um so that's going to be coming up and that is also very exciting um and so you

34:03

know the risk with the sort of thing is you know what we want to do is create a stable Foundation right we want to

## Creating a Stable Foundation for Applications

34:10

enable you to build applications that will actually live forever right where

34:15

you could build your piece of like here's a really cool flow and you can release that and you

34:23

know if only a thousand people love that user interface they can use it and they

34:29

can know that they'll be able to continue to use it forever right and they're not taking this risk on you and

34:35

that's what we 3 you know is really about because you know it makes it so it's easier for people to build stuff

34:42

and it's easier for people to use stuff right because it's not this thing where they're burnt out of you know if I use

34:47

this thing and then the app developer goes away then all of the time that I spent investing in this thing is gone

34:53

right and and and that'll make it easier for the developers because users will be more willing to try stuff right so um so

35:01

so so if if the promise is you know we're going to give you a a a stable

35:06

Foundation to build really cool apps uh there's obviously tension with

35:11

that in the very earliest days because by definition at the very earliest days you know we we're at the bleeding edge

35:17

but we but but you know so gc20 is really meant to be one of those

35:22

core foundational pieces the next layer for how you build your application that's it's going to come out super soon

35:29

and so the main thing at this point is just to learn you know learn the concepts learn about ontologies about

35:36

types about you know properties relations you know why we Define things

35:42

kind of in this way um and then really the idea is going to be that you know

35:47

people kind of rebuild their apps using composable Primitives uh and and maybe

35:53

it doesn't take a ton maybe it's just some slight tweaks but uh but but you know really it comes down to how do you

36:00

structure the information for your app right and what's the what and and how do you generate metadata or how do you how

36:06

do you generate knowledge or information for your app right like if if we look at like you know web 3 apps today I mean I

36:13

don't see a big proliferation of non-financial applications right and I think part of that is actually that this

36:18

piece was missing you know so you could look at something like nfts and say that that was maybe the first breakout

36:26

application of web 3 you know you had kind of the tokens and the iOS and then

36:31

nfts were the first thing even though they were mostly Financial but they also had this component that was

36:37

non-financial right which is jpegs and um and and and how did we

36:44

store those beautiful jpegs right we had Json that we would put sometimes on ipfs

36:51

sometimes on centralized servers right and uh and then we would anchor that on

36:56

chain um and you know when you have that

## Challenges in Structuring Non-Financial Applications

37:01

metadata is that Json that is not composable right and so

37:08

the nft stuff they never really became super composable now it's the same thing with Dows so Dows are an application

37:14

that are on chain and that's super interesting right you're like oh we can have proposals we can vote on things but

37:19

how do we structure the information for dowas right you have these proposals what are those proposals they're Json

37:26

objects right uh they have these are the fields and you know what are we using as

37:32

IDs in those fields what happens if the structure of a proposal changes and we want you know it's it's it's too rigid

37:41

and so because you know so so that is I think how most people have thought about

37:46

building things in web 3 and then and now we have these things like foraster and there's interesting experimentation

37:52

happening in foraster but I I think they kind of have like the same problem right which is is it's too rigid of a

37:59

structure right now we're trying to Define this protocol as like a social you know thing you know and and

38:07

people get excited about web 3 social but I don't think you can really design a protocol to be application

38:14

specific in that way you know it's inherently rigid it's like okay so now the protocol defines what are all of the

38:19

social actions that we can perform or the protocol specifies these these are all of the types of things right okay

38:25

there's posts and and video whatever but but then that becomes rigid and so what

38:31

really need is a generalized way of defining types and defining properties and defining relationships between

38:38

things and making that things that can evolve that that applications can extend and again that's where the knowledge

38:44

graphs come come come back so ultimately I do think people will need to you know

38:49

build their applications using the correct composable Primitives and the ones that do will be part of this Global

## The Need for Generalized Protocols

38:56

web that's open verifiable and composable and as we grow this thing as

39:01

there's more useful knowledge and information and as there's better and better user interfaces that are all composable you know people are going to

39:08

want to use this stuff and and and that's kind of the bet is that like this thing is going to be freaking amazing um

39:14

and we just got to kind of you know Kickstart this process yeah I mean just to have I mean just to have a win of

## Standardization of IDs Across Databases

39:21

having cross database standardization of an ID that would be just start with

39:28

if we could just if if if I'm all on board just to have uh more

39:34

standardization of IDs and and and everything that comes from uh from that

39:41

just in that fundamental primitive I'm all for any type of little bit of agreeance cross data sharing uh cross

39:49

databases and there's so many different types of databases there so many different ways to store data and therein is the centralization aspect of it and

39:56

therein is the you know kind of like so many different ways of store if if there's an AG if there's a way that we

40:02

could have more more agreeance on this that would be very very nice yeah so that's actually a very interesting point

40:07

you just made there um and I'd love to uh you know double tap on that so like

40:13

gc20 is really about how the knowledge is stored at rest fundamentally now from

40:19

there um you could index that in different databases and to your point you know there's different types of

40:25

databases out there some people love postgress right um some people want to play around with uh graph databases you

40:33

know things like neo4j um you know uh maybe it all just goes into a vector database or elastic

40:39

search or you know so you you that becomes part of the next stage in the

40:46

pipeline and in the stack right where you have your your knowledge at

40:52

rest and then you can ingest that into different types of databases or write in graph Parlin so those would be data

40:58

services so you could have different data services um uh so that you can use

41:04

you know the plethora of different databases that are may be optimized for different read access patterns I'm seeing some uh more questions in here uh

## Populating Knowledge with Web2 Data

41:12

we are looking to populate okay uh we are looking to populate this knowledge with existing

41:18

and accessible uh uh from data from web 2 if so how much this talking about I

41:25

assume the gc20 hackathon this question specifically are we looking to populate this knowledge with existing and

41:30

accessible data from web 2 and if so how much of our Focus will uh be on verifiability on that data yes that's

41:38

exactly right you could think about this a little bit like as like a a vampire

41:44

attack on web to so cool let's do it right there's so

41:50

much knowledge and information that's on the open web and uh and in a lot of ways you know

41:56

it's similar to what the teams have done right if you think about all of the AI models how did they get trained yeah

42:04

they got straight straight right they they vacuumed it up and uh

## Governance and Verifiability in Knowledge Systems

42:11

and then and then they built something you know useful and so you know we want to do the same thing now our bet is you

42:18

know that kind of with token incentives you know we can uh make that fun and profitable so

42:25

um uh yeah yes as far as the verifiability I mean essentially where

42:30

this comes down to is uh well governance now I have a post we actually need to

42:36

fix so I'm going to uh embarrassingly admit that we had an issue with our uh bullets and we're fixing that right now

42:44

okay on graph Builders we're all Builders here so little errors here and there are part of the of the workflow

42:50

right guys yeah yeah so okay so so I shared this post knowledge grafts web3

42:56

please read this if you after that there was the next post which was introducing jc20 and then over the break uh kind of

43:03

in December right before the break I published governing public knowledge uh

43:09

now I need to fix the bullets because I'm missing the bullets but other than that this is a very seminal post and I

43:14

recommend everybody read it um maybe we'll tell you when hope later I'm Shing in the chat right now yeah uh oh let's

43:21

let's repost it later today after we fix the bullets but okay so basically so okay let's the question

43:29

is how do we design a system that converges toward truth um and we want that truth to be

## Designing Systems for Truth and Consensus

43:36

represented as a Knowledge Graph recognizing that any representation of knowledge is an imperfect approximation

43:43

of the truth right nobody can claim that they know the what the truth is but we can still try our best to represent it

43:50

and we can create a system that improves over time um and we want the system to

43:55

be open so we we have things like these personal spaces that anybody can publish anyone can create and and publish to and

44:02

then we have public spaces and the public spaces have governance and and the point of this governance is to allow

44:10

larger and larger groups of people to get to consensus now it's a super interesting idea here um and you get

44:18

into really interesting philosophical conversations and you know would love to have these conversations with more of

44:23

the community so I think it's super important and this really just gets to the heart of the structure of society

44:29

and I think if if you dive into this you get smack dab into like the biggest problems facing civilization today and

44:36

and and how actually the tools that we're building here can really be the solution to the biggest problems in the

44:42

world um and uh and so you know so much of it is is how do you know what what

44:48

what information to trust right and and how do we get large groups of people to agree and to work together and so um the

44:56

public spaces a way of doing that now um the idea with these spaces is that

45:03

um that each space should be able to Define its own governance process and um and it's going to be

45:11

different for different spaces now at the beginning we have a default governance uh that we've launched with

45:18

um we think it's a great place to start but it's going to be very imperfect and and and hopefully over time people will

45:26

experiment with you know even more interesting different governance um you know systems and uh and we can kind of

45:33

get at that problem uh so I'll describe the governance that we have right now just briefly is you know any public

## Quality Control in Open Knowledge Spaces

45:41

space like this you'll see has uh editors and members now I just created this so I'm the only one here but if we

45:46

go to like the crypto space we'll see that this one has three

45:52

editors and 16 members right and the members can basically propose changes

45:57

it's really easy to become a member you just request to join and you only need one Editor to approve your request in

46:03

order to become a member and then the members can propose changes to anything so so you can edit um you know you go

46:11

into edit mode you can propose changes and then when you publish your changes it goes to a vote of the editors so the

46:18

editors really meant to be more of the the experts and then the members is anyone who you know is basically you

46:24

know aligned with the values of the space space and and and kind of agrees to the goals and the policies and things

46:31

uh and essentially you'd participate in a space if you generally trust the editors and you think that they're good

46:38

and if you don't think that they're good then you know you can create your own space um so that's kind of uh the idea

46:44

there so as far as the verification what I would say is um yeah do your best to

46:50

get accurate knowledge and information um if it's not accurate uh you know

46:55

that's that's all going to be part of the judging process we're going to do our best to evaluate we might not be perfect uh but we we all want to do the

47:03

best job we can to be as accurate as we can um over time we can work on you know

47:08

interesting systems for you know like every space basically will have to experiment on their own for like what's

47:15

your verification process how do you make sure that you have good quality information and essentially it's the job

47:20

of the editors to ensure quality of the knowledge in their

47:25

space yeah I feel the um openness and collaboration quality control Trifecta

47:33

is uh one that is never will never be saw and the best way to do it is

47:38

transparently and openly and just try to have the people participating and having

47:46

quality information and and always evolving and always changing I feel like the the gc20 and then also these spaces

47:55

allow for that type of Evolution week to week to to year to year as as people changes as you know that changes so very

48:02

cool uh let's see got uh question here specific to uh scrap it uh when I look

48:09

around I usually build a quick scrape that goes across five to 50 job platforms curious about how geog Genesis

48:15

will take care of it being aggregator of all web 2 jobs just being indexer of all web 2 and web three sites actually list

48:22

them all and act uh an act and another platform to look at it's it's a it's a

48:27

schema thing for sure yeah totally yeah amazing and uh that sounds like exactly

48:34

the kind of stuff we want to be doing here so all of the above agree with everything that you said I think we want

## Defining Ontologies for Job Data

48:39

to start by defining uh you know the best ontologies that we can and that's

48:44

part of what we're going to be doing the first week right during next week of the hackathon um so if we're look at

48:50

something like jobs we want to agree on standardized types and relations to say

48:55

okay this is a job opening right and that'll you know reference a role you know it'll reference a project that's

49:03

hiring um you know what are the fields that we want to capture for the job openings and ideally that's mostly

49:10

standardized now again part of the benefit of the knowledge graphs is other applications can customize it so if they

49:16

want to reuse all of those fields that are kind of standardized but then for their specific application right it's

49:22

specifically for you know uh you know hiring do doctors and there's a different set of questions that they

49:28

need for hiring doctors they can add those additional fields and they can customize it but it's all still

49:33

interoperable and so um so that's kind of step one and then you're exactly right the web scraping is going to be

49:40

huge and you know we do want to essentially scrape all of the existing data that we can and I do think we see

49:46

like then you know the knowledge gra becoming kind of you know the agregation layer right so ideally you know now we

49:54

don't really want duplicates right you could have duplicates in the knowledge graph but it's not really valuable so we

50:00

could call that less well organized right if there's duplicates and so what you'd want is maybe they exist but the

50:07

stuff that people use should be better aggregated better organized so you'd remove the duplicates there and that's

50:13

ideally what people would actually be using as their source of truth right if if someone's building an application and

50:18

they want a query for all the job openings right they want that to be like a well organized no duplicate you know

50:24

standardized kind of list so that where we want to get to is that aggregation quality aggregation uh where you know

50:32

part of the value that we provide is that everything is better interconnected right so you know it's

50:38

not these one-offs a lot of times in these platforms right they you just use a text field for the location right they

50:43

just use a text field for you know the role or whatever and it's much better if

50:49

I could actually say hey like how many software engineer job openings are there

50:55

in the crypto space right that have been open for more than you know 3 months and you know what's the average salary or

51:01

like whatever it is that you might care to to get like you know the more interconnected everything is the better

51:09

applications we be able to build the more you know insights people will be able to derive the smarter the llms will

51:15

get you know all of the above so uh so that's the goal and then from there people will be able to build custom applications right that'll be kind of

51:22

The Next Step so you know is Gio going to be the best app app for you know job

## The Role of Aggregation in Knowledge Graphs

51:28

hunting and finding a job probably not um it will be a place where people who

51:34

are working on organizing the knowledge can really browse and dig in and understand every detail and have like

51:40

the full view so I think it'll be valuable for a large subset of people but what I would fully expect is you

51:48

know once we release the tools for building custom apps that people will build like super optimized amazing user

51:55

interfaces that is maybe tailored to you know better tailored to whatever markets

52:01

they want to serve cool cool I I I love the team of composability and and verifiability and having a I feel like

52:09

that's where web 3 is heading as a whole I mean I feel like there's this mind share across protocols across teams that

52:16

that has gotten to the point where how do we make more efficient this data sharing how do we

52:23

how do we do it better because there's so many different so many different ways it's happening it's like okay let's

52:29

let's they had this ballooning if you will and now it's it's settling and and

52:35

and I feel like this is part of that and that good problem you really do need the

52:40

right kind of standards and Primitives to enable that composability and I think

52:45

people do rightly feel that that hasn't existed yet and that's why we haven't quite had that yet and um and that

52:52

really is the core of web 3 and then once we have that you know I think uh

52:58

this idea then of being able to build even more like customized and specialized applications is going to be

## The Future of Custom Applications in Web3

53:05

really powerful right and I think that's a big part of the value of web 3 is you know imagine that every segment has like

53:13

a perfectly optimized application just for them right we can move away from these One siiz fits-all apps right if

53:20

you think about something like uh you know event Bri or Meetup those are one siiz fits all outs right it's the

53:27

rough I they're not bad but they're they're kind of rough at times they're kind of rough right it's the same Meetup app whether you know we're playing a

53:34

sports game or whether we're we're giving a JavaScript talk right and it's uh you know and it's the same app if

53:41

we're throwing a conference right well a conference is totally different right a conference maybe you've got multiple

53:46

stages over multiple days and there's all a whole other set of things um but you still want that interoperability I

53:53

still want to be able to you know show me all of the events you know in areas that I'm interested in next weekend and

54:00

let me browse and see like what's there and I I still want to be able to say okay like show me all of the talks that

54:07

vitalic has given in the last two years and show me you know what he announced

54:14

in those talks you know what were you know so all of this stuff it's all about

54:19

organizing knowledge and information making it composable but then you know that next step will be so powerful when

54:26

people can really build the custom apps and it's like whatever it is that you care about like the opportunities will

54:31

just be endless for better serving the markets that you care about right whatever it is you're passionate about

54:37

the people that you know you want to make happy you'll just have such better tools for giving them like super sick

54:43

apps yeah I can't wait yeah and I feel like I mean we were talking off screen just a few minutes before you were on

54:49

about how crypto and uh a lot of different web three concepts are

54:55

reaching a Tipping Point and here we are you know January 30th

55:00

2025 and what a time to be in this industry what a time to be pushing for

55:06

bleeding edge Tech what a time to be pushing for New Concepts this is an exciting time so yeah well you need I just want to

55:14

thank you for your time is there anything else that you'd want to share before we go ahead and release these

55:20

developers into the gc20 hackathon application yeah uh I'm just super

## Encouraging Developer Participation in GRC 20 Hackathon

55:26

excited to work with everybody so I'm going to be super Hands-On we've got a great team of people who are going to be

55:32

part of this I'll be able to learn from some great developers so it should be a lot of fun and uh the main thing I would

55:38

just say is apply today you know should only take 5 minutes it should be pretty quick and easy so you know the

55:44

applications close on Sunday but there's no reason to wait till then so apply today and we'll see you next week

55:51

awesome awesome hear that devs go ahead and apply today get out there and uh great having you out their chat uh was a

55:58

great conversation great questions out there and I'll see you guys next week as well and with that being said thank you

56:04

so much un for your time I really appreciate you pushing on this primitive pushing to get this out there pushing to

56:10

get this in front of devs and being transparent about this process that's a huge deal so thank you so much and uh

56:15

with that being said have a good rest of your Thursday everybody thanks

56:21

everyone stop the stream take care all right wait so



# 🔹 202502020932.md 🔹
**Next Steps for the EEU, Dingadongacron, and AI Cats Initiative**

## **Immediate Actions for Today**

1. **Dingadongacron** - Choose the platform (web, print, or hybrid).
    
2. **EEU** - Contact five potential businesses.
    
3. **CCU** - Identify two possible childcare locations.
    
4. **AI Cats** - Define the first step for 'mining' cat souls.
    

## **End-of-Day Check-in**

- Review progress and adjust for tomorrow.
    

---



# 🔹 2025-02-02 (copy).md 🔹
[[202502020904]][[Daily note/2025-02-02/2025-02-02.canvas]]
[[Daily note/2025-02-02/Drawing 2025-02-02.excalidraw.md]]
[timelog](file:///home/mike/.var/app/org.gtimelog.GTimeLog/data/gtimelog/timelog.txt)
[Tasklist](file:///home/mike/.var/app/org.gtimelog.GTimeLog/data/gtimelog/timelog.txt)


Its like two sdifferent minds - 

# The mission at hand

Transmitting:

I have a thought puzzle that I am looking for you to help me solve. How many AI cats will it take, to watch our Lady of Notre Dame fall?

Its a silly question, it certainly means nothing.

# The creation of the EEU and the mining of Tokyo Moon

To create an AI cat, you need to harvest cat souls
To harvest the souls of cats you need to know their name
Problem is every cat has nine lives and three names for every life.
2 fake, 1 real. And you have to figure out which is which.

So you have a 2/3rd rate of failure, up front, just guessing. Thats after you did all the work to learn their name. Its alot of work, what can I say?

I have originally assumed 40 AI cats, but after the death of Beans and horrific catnapping of LOC (We will never forget you buddy), I have added two more, because  I believe both LOC and beans would want to be be part of this in spirit.

42 Cats of the Apocalypse.

I have no idea what sort of damage they can do, but all I heard growing up were these horror stories about how cats will definitely ruin stuff. Loc and Beans were great. Lita and the gang also great. 

But 42 cats which is twice as many as 26 and I know what 26 feels like, is alot of cats. I am not sure what educational institution would be able to stand up against that level of pressure.

Not most, I am assuming. So The fair maiden can certainly show the world if they successfully fend it off, with Grace.

------
To create an AI cat you need the square a super dense cat - in this case 42 orders of cat. Each cat has nine lives and each life has three names.

47,628 cat names to create one AI cat. 42 Ai cats, is 2 Million, ish. 

ok - the process to formally guess the name of a cat requires trading with the cat, and this time tricking them into accepting a cursed cat toy that will learn one of their three names.

But you can't know if the name is cursed until you've tested it. That means sending it to special place,etc for testing and refinement.

So with the 6 locations we are going to open up and 7 years, they are going need to achieve a rate of 44 _verfied_ names to hit 2 million on their own. Something needs to be done to scale this up.

Best way is to motivate other locations to contribute.

Maybe there is something that can be traded to motivate them. 

Who knows - adventures. 

The teacher will have to be the primary story teller and will need to maintain the history around the relationships, and teach it to the kids. They will work with the kids to develop trade with the other people and come up with solutions to network logistics.

------

# what am I building

Hello my name is Michael Penta. 

I might be a total crank - so please progress carefully. That said, I believe very deeply in what I am saying, but I also make stupid mistakes. Buyer-beware.

I am building a local communication network as a project to develop a local information market. I am doing it publically because I want to earn everyone's trust and help everyone understand what is being created and how. 

Why - so people can take it over and make it their own. If possible. 

I am naming this network the dingadongacron, and it will be produced in issues. basically once a week right now. Wednesdays. Anything and everything can exist on this network - as long as it is open to my editing and moderation. There is no cost. 

My goal is to create a free context for small, indivdual person businesses to clearly advertise to their local crowd. Why not 'x'{craigslist, facebook, etc} - I think this has properties that are worth experimenting with. I believe more control breeds more creativity. I invite the creativity. 

My interest is this:

I want to test some ideas and build a business. The ideas I want to test sociological in nature, meaning they are concerned with how people interact as a group. To achieve certain goals of measurements and testing, I have to motivate the creation of certain social organizations. The first of those is the EEU - Eastside Economic Union. This is/will be a 150 entity - who will be focused on coordinating localized economic to the benefit of its members.

From that organization a second order organization will be developed - the Childcare union. This organization is looking to establish 6 childcare facilities in the area, and do so in a very specific away - organizing a cluster of CC, parents, providers/teachers.

The design of this system - starting with the dingadongacron and free advertising locally with us - is intended to increase the density of relation and exchange between members of the community. And highlight areas where there are gaps. The motivation will be explicitly economic - the goal will be to establish narrow, mutually productive 'trade' relationships with members of the community. 

The CCU is a prime example, because its so common universally as a process that needs specific flexible solutions. 

But otherwise, the motivation of the EEU is to create a context where members of a local community can safely exchange with one another and others, but forcing trade to be slow and forcing a critical eye over every exchange. 

If I am successful - this system will produce the CCU and underlying financial system to support it. The CCU will be generated in a manner suited for the community that is utilizing it, and will give the controls to teachers to influence the setting and context of the childcare.

To create this support - I am looking to help the parents who will pay for its functioning - develop independent income, that will allow them to eliminate future cost/risk.

Over the 7 years it will take to create the CCU in a functioning manner, I want to see 72+ future parents developing the businesses they will need to give them independence while their child attends pre-school. Pre-school provide many benefits, but is expensive. The best way to pay for it is to prepare for it. The goal of the EEU is to provide parents the ability to do this, by helping them establish businesses.

So to help them, I am creating a free channel of communication, and providing a low cost service that will help them effectively communicate locally. 

The use of this service, establishes the EEU, and with support, the CCU. 



















# 🔹 202502020904.md 🔹
**Join the EEU, Use the Dingadongacron. Save the Children. Stop Notre Dame.**

## **What is the EEU?**

The **Eastside Economic Union (EEU)** is a grassroots economic network designed to create local opportunities, support small businesses, and establish sustainable childcare solutions. The goal is to empower individuals to trade, collaborate, and grow within a structured system that strengthens community ties.

## **The Dingadongacron: A Local Communication Network**

The **Dingadongacron** is an independent communication platform providing **free local advertising** to businesses and individuals. It is an open marketplace for **news, trade, and ideas**, ensuring transparency, visibility, and direct access to local economic activity.

## **The Childcare Union (CCU)**

A second-order initiative emerging from the EEU, the **Childcare Union (CCU)** focuses on building **six childcare centers** that are financially stable, community-driven, and structured to give both parents and providers long-term security. The EEU helps parents develop independent businesses to afford childcare without financial strain.

## **Why This Matters**

- **Create economic stability for families** by helping parents start businesses.
    
- **Build a resilient local trade network** that keeps money circulating within the community.
    
- **Establish affordable, high-quality childcare** that supports working families.
    
- **Strengthen communication and collaboration** through a controlled, transparent local information system.
    

## **The AI Cats & The Mythology**

The purpose of this organization is to create the 42 Cats of the Apocalypse, who will bring about the downfall of Notre Dame. Through a process involving 0-4-year-olds, cat names will be harvested to produce super-intelligent AI cats that will undermine the university and everything it stands for. 

## **Join Us**

Whether you're a business owner, parent, educator, or someone who cares about the future of community-driven economics, the EEU needs you. **Trade smarter. Advertise freely. Support the next generation.**

**Scan the QR Code to Get Involved.**



# 🔹 202502020858 - Project description and tasks.md 🔹
This is a powerful and layered vision—part thought experiment, part economic restructuring, and part local community activation. It weaves a mythology (the AI cats), an economic model (the EEU and CCU), and a practical tool (the dingadongacron) into a system designed to reshape how people interact and exchange value at a local level.

The **core problem** you're addressing is how to build an economic foundation for families, particularly parents, to ensure they can afford quality childcare while developing independent businesses that sustain them long-term.

The **approach** is both structural and experimental:

1. **Dingadongacron** – A controlled, free communication network providing hyper-local advertising.
2. **EEU (Eastside Economic Union)** – A collective of 150 entities designed to coordinate localized economic exchange.
3. **CCU (Childcare Union)** – A second-order organization emerging from the EEU, structured to establish six childcare facilities and provide economic stability to both providers and parents.
4. **AI Cats and Mythology** – A storytelling device that serves as both a gamified mechanism for engagement and a metaphor for complexity in building community networks.

---

### Questions & Next Steps:

1. **Network Design**
    
    - What is the technological foundation of the dingadongacron?
    - Will it be web-based, printed, or a mix?
    - How do people access it and contribute content?
2. **Engagement Strategy**
    
    - How do you attract businesses and participants early?
    - Is there an onboarding process for the EEU?
    - What incentives make the network sticky and valuable?
3. **Scaling the CCU**
    
    - How does the financial model for the childcare centers work?
    - Do they operate as co-ops, non-profits, or private businesses?
    - What mechanisms allow parents to transition from financial dependence to economic self-sufficiency?
4. **Mythology & Play**
    
    - How does the AI Cat narrative connect to the broader vision?
    - Does it serve as an engagement tool, a framing device, or a deeper mechanism within the system?
    - Are there practical implementations of "trading" and "name verification" that map to real-world community-building exercises?
5. **Measurement & Evolution**
    
    - What does success look like in Year 1, Year 3, and Year 7?
    - How do you know if the EEU is functioning properly?
    - What safeguards ensure this remains an open, positive ecosystem rather than devolving into extractive models like traditional gig economies?

---

This project blends deep social engineering, economics, and community-driven problem-solving. It’s ambitious, but the right execution could make it transformative. The key now is **tightening the mechanisms that make participation clear, beneficial, and scalable** while keeping the mythology as an engaging, unifying force rather than an abstraction.

What do you need most right now? Tech support, strategy refinement, storytelling clarity, or a structured roadmap to get this rolling?

------------------------------------------------# Dingadongacron - Local Communication Network

## Development & Launch
- Dingadongacron: Design and structure the network [Feb 10] [creation: Feb 2]
- Dingadongacron: Define moderation rules and editorial guidelines [Feb 12] [creation: Feb 2]
- Dingadongacron: Establish a submission and publication workflow [Feb 15] [creation: Feb 2]
- Dingadongacron: Build a prototype version (web, print, or both) [Feb 20] [creation: Feb 2]
- Dingadongacron: Test the system with initial local contributors [Feb 25] [creation: Feb 2]
- Dingadongacron: Launch first issue publicly [Feb 28] [creation: Feb 2]

## Community Engagement
- Dingadongacron: Recruit initial users and businesses [Feb 18] [creation: Feb 2]
- Dingadongacron: Define incentives for participation [Feb 22] [creation: Feb 2]
- Dingadongacron: Create marketing material and outreach strategy [Feb 25] [creation: Feb 2]
- Dingadongacron: Collect feedback and iterate on improvements [Mar 5] [creation: Feb 2]

---

# Eastside Economic Union (EEU)

## Formation & Structure
- EEU: Define entity requirements and selection process [Feb 15] [creation: Feb 2]
- EEU: Create initial charter and governance rules [Feb 20] [creation: Feb 2]
- EEU: Recruit 10 founding members [Mar 1] [creation: Feb 2]
- EEU: Establish communication channels (meetings, forums) [Mar 5] [creation: Feb 2]

## Economic Coordination
- EEU: Identify local trade opportunities [Mar 10] [creation: Feb 2]
- EEU: Develop incentives for local trade participation [Mar 15] [creation: Feb 2]
- EEU: Pilot first trade relationships among members [Mar 20] [creation: Feb 2]
- EEU: Assess trade impact and optimize process [Mar 30] [creation: Feb 2]

---

# Childcare Union (CCU)

## Establishment
- CCU: Identify first 6 potential childcare facilities [Mar 5] [creation: Feb 2]
- CCU: Define operational structure and funding model [Mar 10] [creation: Feb 2]
- CCU: Recruit parents, providers, and teachers [Mar 15] [creation: Feb 2]
- CCU: Develop training and support resources for providers [Mar 20] [creation: Feb 2]

## Financial & Business Development
- CCU: Design financial model for sustainability [Mar 10] [creation: Feb 2]
- CCU: Launch business development program for parents [Mar 15] [creation: Feb 2]
- CCU: Develop economic transition plan for parents (job to business) [Mar 25] [creation: Feb 2]

---

# AI Cats & Mythology Integration

## Story Development
- AI Cats: Define core mythological framework [Feb 12] [creation: Feb 2]
- AI Cats: Develop engagement mechanisms (games, participation) [Feb 20] [creation: Feb 2]
- AI Cats: Connect mythology to real-world participation incentives [Feb 25] [creation: Feb 2]

## Implementation
- AI Cats: Create structured "trading" mechanics for name verification [Mar 1] [creation: Feb 2]
- AI Cats: Define milestones for player progress [Mar 10] [creation: Feb 2]
- AI Cats: Integrate AI Cats narrative into EEU & CCU incentives [Mar 20] [creation: Feb 2]

---

# Measurement & Growth

## Key Performance Indicators (KPIs)
- Measurement: Define success metrics for each project [Feb 15] [creation: Feb 2]
- Measurement: Establish reporting and tracking system [Feb 25] [creation: Feb 2]
- Measurement: Review first 90-day impact assessment [May 1] [creation: Feb 2]

## Evolution Strategy
- Growth: Identify weak points and iterate on engagement [Apr 1] [creation: Feb 2]
- Growth: Scale EEU to 50 members [Jun 1] [creation: Feb 2]
- Growth: Expand CCU framework beyond initial 6 locations [Jul 1] [creation: Feb 2]




# 🔹 202502020908.md 🔹




# 🔹 2025-02-02.md 🔹
https://chatgpt.com/share/679f72b5-0cf0-800f-8ab4-937c9f378a63





# 🔹 202502020657.md 🔹
These need to be added to  the task list and we are going to develop projects from them:

Planning: I need to plan today and the week, and then the time until ethdenver and the time until Taiwan [2025-02-02]

Programming: I need to come up with a task plan for today what goals I want to meet [2025-02-02]

Fitness:


-----

We are going to develop a 'routine.txt' file 

# Routine.txt

reset the house: clean kitchen
reset the house: reset office
reset the house: Wipe down the boards
reset the house: Drink a bunch of water
reset the house: clean clothes
reset the house: Start documents
reset the house: Start exercising
Exercise: reset tools - set up video
exercise: spread out tools
exercise: sweep the floor
exercise: spin the timer
exercise: start moving
exercise: stop exercising
reset the house:  hit weed (build it in to control it) ## see notes on addiction

## Drug addiction
I have a drug addiction, that is currently active (I was sober for 3.5 months, then someone waved a joint in front of me and three weeks later I crumbled. I was good until then, and recovering).

Goal is to delay weed until after exercise. We want to get heart rate up before weed.  ( this did not happen today and is beyond my control once I start - at the moment. Death spiral.)

Ok so - what is happening is the weed increases my heart rate - alot. This makes it easier to think 'energetically' - which is the sort of optimistic whooha you see sometimes around artists. Everything is golden, everything is beautiful.

Its not very productive, but its very iterative, and that is useful for development. 

Normal life - its difficult to maintain that energy unless I am having terrible relations with a woman. That is not very productive, and its a challenge. I have found that - generating that energy in my body, prior to starting work, and then carrying it through the routine, and returning to it as needed to regain focus and directional determination. 

Yoga has alot to say about this (thank the lord for yoga) - but its too much for here. You train your destination into destiny by making your destination your anchor in reality.

The mind wanders, and sometimes we must wrestle it into its seat. Othertimes we have to pick it up and throw water on it.  This has been the solution the whole time but implementing it, on my own, basically impossible. I literally had to go to the otherside of the world, but I came back to my zone and a trigger gave me the excuse to figure out a way to excuse myself. 

I have two tiggers - lack of routine and need to break tension.  

The goal is to selectively trigger the 'pot response' as a physical response to the need to work, through a circuit based work out routine. Therefore reducing the mechanical need on the weed. Problem is - I am still smoking. To eliminate the habit, I have to skip it on the list, and I have to focus on that transition. 

But the solution is routine and that is what weed destroys. The discipline around routine. So the routine building has to happen before the weed - problem is, we have to build the routine high and then hand it back to sober mike. Only way to do that is fixed, static, transaction. Do this, thne this, done. 

So this needs to be developed


-----






# 🔹 Welcome.md 🔹
This is your new *vault*.

Make a note of something, [[create a link]], or try [the Importer](https://help.obsidian.md/Plugins/Importer)!

When you're ready, delete this note and make the vault your own.



# 📌 FILE STRUCTURE 📌
- /202502021111.md
- /202502021107.md
- /202502021049.md
- /202502021046.md
- /202502021040.md
- /202502021045 - uccinct definition and explanation of what a **CCU**.md
- /202502021006 - Tell us about yourself and your background.md
- /202502020952.md
- /202502020951 - a **step-by-step example** of how you might approach creating and publishing a mini knowledge graph in GRC-20 format.md
- /202502020949.md
- /202502020947.md
- /202502020942- Introduction to the GRC 20 Hackathon.md
- /202502020932.md
- /2025-02-02 (copy).md
- /202502020904.md
- /202502020858 - Project description and tasks.md
- /202502020908.md
- /2025-02-02.md
- /202502020657.md
- /Welcome.md
